{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training and Evaluation: Individual Models\n",
        "\n",
        "================================================================================\n",
        "PURPOSE: Train and evaluate classifiers on features from each model separately\n",
        "================================================================================\n",
        "\n",
        "This notebook trains multiple classifiers on Context Tree features extracted\n",
        "from individual transformer models. Each model (BERT, RoBERTa, DeBERTa, XLNet)\n",
        "is evaluated separately to assess their individual performance on the clarity\n",
        "and evasion classification tasks.\n",
        "\n",
        "**Workflow:**\n",
        "1. Load features from Google Drive (saved by 02_feature_extraction_separate.ipynb)\n",
        "2. Train multiple classifiers on each model's features\n",
        "3. Evaluate on Dev set (model selection and hyperparameter tuning)\n",
        "4. Save predictions and probabilities for further analysis\n",
        "5. Generate comprehensive results tables and evaluation plots\n",
        "\n",
        "**Classifiers:**\n",
        "- Logistic Regression\n",
        "- Linear Support Vector Classifier (LinearSVC)\n",
        "- Random Forest\n",
        "- XGBoost\n",
        "- LightGBM\n",
        "\n",
        "**Output:** \n",
        "- Predictions (hard labels) and probabilities saved to Google Drive\n",
        "- Results tables comparing classifiers for each model/task combination\n",
        "- Evaluation plots (confusion matrices, PR curves, ROC curves)\n",
        "- Results metadata saved for final summary generation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# SETUP: Repository Clone, Drive Mount, and Path Configuration\n",
        "# ============================================================================\n",
        "# This cell performs minimal setup required for the notebook to run:\n",
        "# 1. Clones repository from GitHub (if not already present)\n",
        "# 2. Mounts Google Drive for persistent data storage\n",
        "# 3. Configures Python paths and initializes StorageManager\n",
        "# 4. Loads data splits and features created in previous notebooks\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "import zipfile\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "\n",
        "# Repository configuration\n",
        "repo_dir = '/content/semeval-context-tree-modular'\n",
        "repo_url = 'https://github.com/EonTechie/semeval-context-tree-modular.git'\n",
        "zip_url = 'https://github.com/EonTechie/semeval-context-tree-modular/archive/refs/heads/main.zip'\n",
        "\n",
        "# Clone repository (if not already present)\n",
        "if not os.path.exists(repo_dir):\n",
        "    print(\"Cloning repository from GitHub...\")\n",
        "    max_retries = 2\n",
        "    clone_success = False\n",
        "    \n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            result = subprocess.run(\n",
        "                ['git', 'clone', repo_url],\n",
        "                cwd='/content',\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                timeout=60\n",
        "            )\n",
        "            if result.returncode == 0:\n",
        "                print(\"Repository cloned successfully via git\")\n",
        "                clone_success = True\n",
        "                break\n",
        "            else:\n",
        "                if attempt < max_retries - 1:\n",
        "                    time.sleep(3)\n",
        "        except Exception as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(3)\n",
        "    \n",
        "    # Fallback: Download as ZIP if git clone fails\n",
        "    if not clone_success:\n",
        "        print(\"Git clone failed. Downloading repository as ZIP archive...\")\n",
        "        zip_path = '/tmp/repo.zip'\n",
        "        try:\n",
        "            response = requests.get(zip_url, stream=True, timeout=60)\n",
        "            response.raise_for_status()\n",
        "            with open(zip_path, 'wb') as f:\n",
        "                for chunk in response.iter_content(chunk_size=8192):\n",
        "                    f.write(chunk)\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall('/content')\n",
        "            extracted_dir = '/content/semeval-context-tree-modular-main'\n",
        "            if os.path.exists(extracted_dir):\n",
        "                os.rename(extracted_dir, repo_dir)\n",
        "            os.remove(zip_path)\n",
        "            print(\"Repository downloaded and extracted successfully\")\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Failed to obtain repository: {e}\")\n",
        "\n",
        "# Mount Google Drive (if not already mounted)\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "except Exception:\n",
        "    pass  # Already mounted\n",
        "\n",
        "# Configure paths\n",
        "BASE_PATH = Path('/content/semeval-context-tree-modular')\n",
        "DATA_PATH = Path('/content/drive/MyDrive/semeval_data')\n",
        "sys.path.insert(0, str(BASE_PATH))\n",
        "\n",
        "# Initialize StorageManager and load required modules\n",
        "from src.storage.manager import StorageManager\n",
        "from src.models.trainer import train_and_evaluate\n",
        "from src.models.classifiers import get_classifier_dict\n",
        "\n",
        "storage = StorageManager(\n",
        "    base_path=str(BASE_PATH),\n",
        "    data_path=str(DATA_PATH),\n",
        "    github_path=str(BASE_PATH)\n",
        ")\n",
        "\n",
        "# Load data splits for label extraction\n",
        "train_ds = storage.load_split('train')\n",
        "dev_ds = storage.load_split('dev')\n",
        "\n",
        "print(\"Setup complete\")\n",
        "print(f\"  Repository: {BASE_PATH}\")\n",
        "print(f\"  Data storage: {DATA_PATH}\")\n",
        "print(f\"  Train samples: {len(train_ds)}\")\n",
        "print(f\"  Dev samples: {len(dev_ds)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CONFIGURE MODELS, TASKS, AND CLASSIFIERS\n",
        "# ============================================================================\n",
        "# Defines the models to evaluate, tasks to perform, and classifiers to train\n",
        "# Label mappings are defined for clarity (3-class) and evasion (9-class) tasks\n",
        "\n",
        "MODELS = ['bert', 'roberta', 'deberta', 'xlnet']\n",
        "TASKS = ['clarity', 'evasion']\n",
        "\n",
        "# Label mappings for each task\n",
        "CLARITY_LABELS = ['Clear Reply', 'Ambiguous', 'Clear Non-Reply']\n",
        "EVASION_LABELS = ['Direct Answer', 'Partial Answer', 'Implicit Answer', \n",
        "                  'Uncertainty', 'Refusal', 'Clarification', \n",
        "                  'Question', 'Topic Shift', 'Other']\n",
        "\n",
        "# Initialize classifiers with fixed random seed for reproducibility\n",
        "classifiers = get_classifier_dict(random_state=42)\n",
        "\n",
        "print(\"Configuration:\")\n",
        "print(f\"  Models: {MODELS}\")\n",
        "print(f\"  Tasks: {TASKS}\")\n",
        "print(f\"  Classifiers: {list(classifiers.keys())}\")\n",
        "print(f\"  Clarity labels: {len(CLARITY_LABELS)} classes\")\n",
        "print(f\"  Evasion labels: {len(EVASION_LABELS)} classes\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# TRAIN AND EVALUATE CLASSIFIERS FOR EACH MODEL AND TASK\n",
        "# ============================================================================\n",
        "# Iterates through each model and task, trains all classifiers, and evaluates\n",
        "# on the Dev set. Results are saved for later analysis and final summary generation.\n",
        "\n",
        "all_results = {}\n",
        "\n",
        "for model in MODELS:\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"MODEL: {model.upper()}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    all_results[model] = {}\n",
        "    \n",
        "    for task in TASKS:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"TASK: {task.upper()}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        # Select appropriate label list and dataset key for this task\n",
        "        if task == 'clarity':\n",
        "            label_list = CLARITY_LABELS\n",
        "            label_key = 'clarity_label'\n",
        "        else:  # evasion\n",
        "            label_list = EVASION_LABELS\n",
        "            label_key = 'evasion_label'\n",
        "        \n",
        "        # Load features from persistent storage\n",
        "        print(\"Loading features from Google Drive...\")\n",
        "        X_train = storage.load_features(model, task, 'train')\n",
        "        X_dev = storage.load_features(model, task, 'dev')\n",
        "        \n",
        "        # Extract labels from dataset splits\n",
        "        y_train = np.array([train_ds[i][label_key] for i in range(len(train_ds))])\n",
        "        y_dev = np.array([dev_ds[i][label_key] for i in range(len(dev_ds))])\n",
        "        \n",
        "        print(f\"  Train: {X_train.shape[0]} samples, {X_train.shape[1]} features\")\n",
        "        print(f\"  Dev: {X_dev.shape[0]} samples, {X_dev.shape[1]} features\")\n",
        "        \n",
        "        # Train all classifiers and evaluate on Dev set\n",
        "        # This function handles training, prediction, metric computation, and visualization\n",
        "        results = train_and_evaluate(\n",
        "            X_train, y_train, X_dev, y_dev,\n",
        "            label_list=label_list,\n",
        "            task_name=f\"{model}_{task}\",\n",
        "            classifiers=classifiers,\n",
        "            random_state=42,\n",
        "            print_report=True,      # Print detailed classification report\n",
        "            print_table=True,       # Print results comparison table\n",
        "            create_plots=True,      # Generate confusion matrices and PR/ROC curves\n",
        "            save_plots_dir=str(DATA_PATH / 'plots')\n",
        "        )\n",
        "        \n",
        "        # Save predictions and probabilities to persistent storage\n",
        "        # These will be used for further analysis and final summary generation\n",
        "        for classifier_name, result in results.items():\n",
        "            # Save hard label predictions\n",
        "            storage.save_predictions(\n",
        "                result['dev_pred'],\n",
        "                model, classifier_name, task, 'dev'\n",
        "            )\n",
        "            \n",
        "            # Save probability distributions (if classifier supports it)\n",
        "            if result['dev_proba'] is not None:\n",
        "                storage.save_probabilities(\n",
        "                    result['dev_proba'],\n",
        "                    model, classifier_name, task, 'dev'\n",
        "                )\n",
        "        \n",
        "        all_results[model][task] = results\n",
        "        \n",
        "        # Save results summary to metadata for final summary generation\n",
        "        experiment_id = f\"{model}_{task}_separate\"\n",
        "        storage.save_results({\n",
        "            'model': model,\n",
        "            'task': task,\n",
        "            'results': {\n",
        "                name: {\n",
        "                    'metrics': res['metrics'],\n",
        "                    'n_train': len(y_train),\n",
        "                    'n_dev': len(y_dev)\n",
        "                }\n",
        "                for name, res in results.items()\n",
        "            }\n",
        "        }, experiment_id)\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"Training and evaluation complete for all models and tasks\")\n",
        "print(f\"{'='*80}\")\n",
        "print(\"\\nSummary:\")\n",
        "print(\"  - All classifiers trained and evaluated on Dev set\")\n",
        "print(\"  - Predictions and probabilities saved to Google Drive\")\n",
        "print(\"  - Results tables and plots generated\")\n",
        "print(\"  - Metadata saved for final summary generation\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
