{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Extraction: Separate Models\n",
        "\n",
        "This notebook extracts 19 Context Tree features for each model separately.\n",
        "\n",
        "**Models:**\n",
        "- BERT (bert-base-uncased)\n",
        "- RoBERTa (roberta-base)\n",
        "- DeBERTa (microsoft/deberta-v3-base)\n",
        "- XLNet (xlnet-base-cased)\n",
        "\n",
        "**Tasks:**\n",
        "- Clarity (3-class)\n",
        "- Evasion (9-class)\n",
        "\n",
        "**Output:** Features saved to Drive for each model/task combination.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup (run 00_setup.ipynb and 01_data_split.ipynb first)\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "BASE_PATH = Path('/content/semeval-context-tree-modular')\n",
        "DATA_PATH = Path('/content/drive/MyDrive/semeval_data')\n",
        "sys.path.insert(0, str(BASE_PATH))\n",
        "\n",
        "from src.storage.manager import StorageManager\n",
        "from src.features.extraction import featurize_hf_dataset_in_batches_v2\n",
        "\n",
        "storage = StorageManager(\n",
        "    base_path=str(BASE_PATH),\n",
        "    data_path=str(DATA_PATH),\n",
        "    github_path=str(BASE_PATH)\n",
        ")\n",
        "\n",
        "# Load splits\n",
        "train_ds = storage.load_split('train')\n",
        "dev_ds = storage.load_split('dev')\n",
        "test_ds = storage.load_split('test')  # Will be used only in final evaluation\n",
        "\n",
        "print(f\"✅ Loaded splits:\")\n",
        "print(f\"   Train: {len(train_ds)} samples\")\n",
        "print(f\"   Dev: {len(dev_ds)} samples\")\n",
        "print(f\"   Test: {len(test_ds)} samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model configurations\n",
        "MODELS = {\n",
        "    'bert': {\n",
        "        'name': 'bert-base-uncased',\n",
        "        'display': 'BERT'\n",
        "    },\n",
        "    'roberta': {\n",
        "        'name': 'roberta-base',\n",
        "        'display': 'RoBERTa'\n",
        "    },\n",
        "    'deberta': {\n",
        "        'name': 'microsoft/deberta-v3-base',\n",
        "        'display': 'DeBERTa'\n",
        "    },\n",
        "    'xlnet': {\n",
        "        'name': 'xlnet-base-cased',\n",
        "        'display': 'XLNet'\n",
        "    }\n",
        "}\n",
        "\n",
        "# Tasks\n",
        "TASKS = ['clarity', 'evasion']\n",
        "\n",
        "# Device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"✅ Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract features for each model and task\n",
        "for model_key, model_info in MODELS.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Processing {model_info['display']} ({model_info['name']})\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Load tokenizer and model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_info['name'])\n",
        "    model = AutoModel.from_pretrained(model_info['name'])\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    \n",
        "    for task in TASKS:\n",
        "        print(f\"\\n--- Task: {task.upper()} ---\")\n",
        "        \n",
        "        # Get labels for this task\n",
        "        if task == 'clarity':\n",
        "            label_key = 'clarity_label'\n",
        "        else:  # evasion\n",
        "            label_key = 'evasion_label'\n",
        "        \n",
        "        # Extract features for each split\n",
        "        for split_name, split_ds in [('train', train_ds), ('dev', dev_ds)]:\n",
        "            print(f\"  Extracting {split_name} features...\")\n",
        "            \n",
        "            # Get texts\n",
        "            questions = split_ds['question']\n",
        "            answers = split_ds['answer']\n",
        "            \n",
        "            # Extract features\n",
        "            X, feature_names, _ = featurize_hf_dataset_in_batches_v2(\n",
        "                split_ds,\n",
        "                tokenizer,\n",
        "                model,\n",
        "                device,\n",
        "                batch_size=8,\n",
        "                max_sequence_length=256,\n",
        "                question_key='question',\n",
        "                answer_key='answer',\n",
        "                show_progress=True\n",
        "            )\n",
        "            \n",
        "            # Save features\n",
        "            storage.save_features(\n",
        "                X, model_key, task, split_name, feature_names\n",
        "            )\n",
        "            \n",
        "            print(f\"    ✅ Saved: {X.shape[0]} samples, {X.shape[1]} features\")\n",
        "    \n",
        "    # Free up memory\n",
        "    del model, tokenizer\n",
        "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"✅ Feature extraction complete for all models!\")\n",
        "print(f\"{'='*60}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
