{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Final Evaluation: TEST Set Only\n",
        "\n",
        "**⚠️ IMPORTANT**: This notebook evaluates on the **TEST set** which was separated at the beginning and never used for training or model selection.\n",
        "\n",
        "**Steps:**\n",
        "1. Load best model/classifier combination (based on Dev set results)\n",
        "2. Extract features for TEST set (if not already done)\n",
        "3. Evaluate on TEST set\n",
        "4. Generate final reports and plots\n",
        "\n",
        "**Note:** TEST set is ONLY used here, never before!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup (run ALL previous notebooks first)\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "BASE_PATH = Path('/content/semeval-context-tree-modular')\n",
        "DATA_PATH = Path('/content/drive/MyDrive/semeval_data')\n",
        "sys.path.insert(0, str(BASE_PATH))\n",
        "\n",
        "from src.storage.manager import StorageManager\n",
        "from src.features.extraction import featurize_hf_dataset_in_batches_v2\n",
        "from src.models.classifiers import get_classifier_dict\n",
        "from src.evaluation.metrics import compute_all_metrics, print_classification_report\n",
        "from src.evaluation.tables import print_results_table\n",
        "from src.evaluation.visualizer import visualize_all_evaluation\n",
        "\n",
        "storage = StorageManager(\n",
        "    base_path=str(BASE_PATH),\n",
        "    data_path=str(DATA_PATH),\n",
        "    github_path=str(BASE_PATH)\n",
        ")\n",
        "\n",
        "# Load TEST split (ONLY used here!)\n",
        "test_ds = storage.load_split('test')\n",
        "\n",
        "print(\"✅ Setup complete!\")\n",
        "print(f\"⚠️  TEST set: {len(test_ds)} samples (ONLY used for final evaluation!)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "# TODO: Select best model/classifier based on Dev set results\n",
        "# For now, using all combinations for comparison\n",
        "\n",
        "MODELS = ['bert', 'roberta', 'deberta', 'xlnet']\n",
        "TASKS = ['clarity', 'evasion']\n",
        "\n",
        "# Label mappings\n",
        "CLARITY_LABELS = ['Clear Reply', 'Ambiguous', 'Clear Non-Reply']\n",
        "EVASION_LABELS = ['Direct Answer', 'Partial Answer', 'Implicit Answer', \n",
        "                  'Uncertainty', 'Refusal', 'Clarification', \n",
        "                  'Question', 'Topic Shift', 'Other']\n",
        "\n",
        "# Get classifiers\n",
        "classifiers = get_classifier_dict(random_state=42)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(f\"✅ Will evaluate on TEST set for all model/classifier combinations\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract TEST features if not already done\n",
        "# (This should have been done in 02_feature_extraction_separate.ipynb, but check first)\n",
        "\n",
        "MODEL_CONFIGS = {\n",
        "    'bert': 'bert-base-uncased',\n",
        "    'roberta': 'roberta-base',\n",
        "    'deberta': 'microsoft/deberta-v3-base',\n",
        "    'xlnet': 'xlnet-base-cased'\n",
        "}\n",
        "\n",
        "for model_key, model_name in MODEL_CONFIGS.items():\n",
        "    for task in TASKS:\n",
        "        # Check if TEST features already exist\n",
        "        try:\n",
        "            X_test = storage.load_features(model_key, task, 'test')\n",
        "            print(f\"✅ TEST features already exist: {model_key}_{task}\")\n",
        "        except FileNotFoundError:\n",
        "            print(f\"⚠️  TEST features not found for {model_key}_{task}, extracting...\")\n",
        "            \n",
        "            # Load model\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            model = AutoModel.from_pretrained(model_name)\n",
        "            model.to(device)\n",
        "            model.eval()\n",
        "            \n",
        "            # Extract features\n",
        "            X_test, feature_names, _ = featurize_hf_dataset_in_batches_v2(\n",
        "                test_ds,\n",
        "                tokenizer,\n",
        "                model,\n",
        "                device,\n",
        "                batch_size=8,\n",
        "                max_sequence_length=256,\n",
        "                question_key='question',\n",
        "                answer_key='answer',\n",
        "                show_progress=True\n",
        "            )\n",
        "            \n",
        "            # Save features\n",
        "            storage.save_features(\n",
        "                X_test, model_key, task, 'test', feature_names\n",
        "            )\n",
        "            \n",
        "            print(f\"    ✅ Saved: {X_test.shape[0]} samples, {X_test.shape[1]} features\")\n",
        "            \n",
        "            # Free memory\n",
        "            del model, tokenizer\n",
        "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "\n",
        "print(\"\\n✅ TEST feature extraction complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final evaluation on TEST set\n",
        "# Load best models from Dev set and evaluate on TEST\n",
        "\n",
        "final_results = {}\n",
        "\n",
        "for model in MODELS:\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"MODEL: {model.upper()} - FINAL EVALUATION ON TEST SET\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    final_results[model] = {}\n",
        "    \n",
        "    for task in TASKS:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"TASK: {task.upper()}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        # Get label list\n",
        "        if task == 'clarity':\n",
        "            label_list = CLARITY_LABELS\n",
        "            label_key = 'clarity_label'\n",
        "        else:  # evasion\n",
        "            label_list = EVASION_LABELS\n",
        "            label_key = 'evasion_label'\n",
        "        \n",
        "        # Get TEST labels\n",
        "        y_test = np.array([test_ds[i][label_key] for i in range(len(test_ds))])\n",
        "        \n",
        "        # Load TEST features\n",
        "        X_test = storage.load_features(model, task, 'test')\n",
        "        \n",
        "        # Load TRAIN features and labels (to retrain on full train+dev)\n",
        "        X_train = storage.load_features(model, task, 'train')\n",
        "        train_ds = storage.load_split('train')\n",
        "        y_train = np.array([train_ds[i][label_key] for i in range(len(train_ds))])\n",
        "        \n",
        "        # Load DEV features and labels (combine with train for final training)\n",
        "        X_dev = storage.load_features(model, task, 'dev')\n",
        "        dev_ds = storage.load_split('dev')\n",
        "        y_dev = np.array([dev_ds[i][label_key] for i in range(len(dev_ds))])\n",
        "        \n",
        "        # Combine train + dev for final training\n",
        "        X_train_full = np.vstack([X_train, X_dev])\n",
        "        y_train_full = np.concatenate([y_train, y_dev])\n",
        "        \n",
        "        print(f\"  Training on: {X_train_full.shape[0]} samples (train+dev)\")\n",
        "        print(f\"  Testing on: {X_test.shape[0]} samples (TEST)\")\n",
        "        \n",
        "        # Train all classifiers on full train+dev\n",
        "        task_results = {}\n",
        "        \n",
        "        for classifier_name, clf in classifiers.items():\n",
        "            print(f\"\\n  Training {classifier_name}...\")\n",
        "            \n",
        "            # Train on full train+dev\n",
        "            clf.fit(X_train_full, y_train_full)\n",
        "            \n",
        "            # Predict on TEST\n",
        "            y_test_pred = clf.predict(X_test)\n",
        "            \n",
        "            # Get probabilities\n",
        "            try:\n",
        "                y_test_proba = clf.predict_proba(X_test)\n",
        "            except AttributeError:\n",
        "                y_test_proba = None\n",
        "            \n",
        "            # Compute metrics\n",
        "            metrics = compute_all_metrics(y_test, y_test_pred, label_list, \n",
        "                                         task_name=f\"TEST_{model}_{task}_{classifier_name}\")\n",
        "            \n",
        "            # Print classification report\n",
        "            print_classification_report(\n",
        "                y_test, y_test_pred, label_list,\n",
        "                task_name=f\"TEST - {model} - {task} - {classifier_name}\"\n",
        "            )\n",
        "            \n",
        "            # Visualize\n",
        "            if y_test_proba is not None:\n",
        "                visualize_all_evaluation(\n",
        "                    y_test, y_test_pred, y_test_proba, label_list,\n",
        "                    task_name=f\"TEST_{model}_{task}\",\n",
        "                    classifier_name=classifier_name,\n",
        "                    save_dir=str(DATA_PATH / 'plots' / 'final_evaluation')\n",
        "                )\n",
        "            \n",
        "            task_results[classifier_name] = {\n",
        "                'metrics': metrics,\n",
        "                'predictions': y_test_pred,\n",
        "                'probabilities': y_test_proba\n",
        "            }\n",
        "            \n",
        "            # Save TEST predictions and probabilities\n",
        "            storage.save_predictions(\n",
        "                y_test_pred, model, classifier_name, task, 'test'\n",
        "            )\n",
        "            if y_test_proba is not None:\n",
        "                storage.save_probabilities(\n",
        "                    y_test_proba, model, classifier_name, task, 'test'\n",
        "                )\n",
        "        \n",
        "        # Print results table\n",
        "        print_results_table(\n",
        "            {name: {'metrics': res['metrics']} for name, res in task_results.items()},\n",
        "            task_name=f\"TEST - {model} - {task}\",\n",
        "            sort_by=\"Macro F1\"\n",
        "        )\n",
        "        \n",
        "        final_results[model][task] = task_results\n",
        "        \n",
        "        # Save final results\n",
        "        experiment_id = f\"FINAL_TEST_{model}_{task}\"\n",
        "        storage.save_results({\n",
        "            'split': 'test',\n",
        "            'model': model,\n",
        "            'task': task,\n",
        "            'n_test': len(y_test),\n",
        "            'results': {\n",
        "                name: {'metrics': res['metrics']}\n",
        "                for name, res in task_results.items()\n",
        "            }\n",
        "        }, experiment_id)\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"✅ FINAL EVALUATION ON TEST SET COMPLETE!\")\n",
        "print(f\"{'='*80}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
