\documentclass[10pt,twocolumn]{article}

\usepackage[margin=0.75in]{geometry}
\usepackage{times}
\usepackage{titlesec}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{float}
\usepackage{placeins}
\usepackage{caption}
\usepackage[section]{placeins}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{amsmath}

% Spacing adjustments
\titlespacing{\section}{0pt}{12pt}{6pt}
\titlespacing{\subsection}{0pt}{8pt}{4pt}
\setlength{\itemsep}{2pt}
\setlength{\parskip}{3pt}

\renewcommand{\textfraction}{0.05}
\renewcommand{\topfraction}{0.95}
\renewcommand{\bottomfraction}{0.95}
\renewcommand{\floatpagefraction}{0.85}
\setcounter{topnumber}{10}
\setlist{leftmargin=12pt}
\setlength{\columnsep}{0.25in}

\title{\textbf{Attention-Based Pragmatic Feature Extraction for Political Evasion Detection}}

\author{Group 2}

\date{}

\begin{document}

\twocolumn[
\maketitle
\vspace{-1em}
]

\section{Introduction}

Political interviews frequently contain deliberately evasive, ambiguous, or strategically reframed answers. While transformer-based models capture rich semantic representations, they may miss fine-grained linguistic and structural cues that signal evasion strategies. This work introduces an \textbf{Attention-Based Pragmatic Feature Extraction} methodology that systematically leverages transformer attention mechanisms to extract 25 interpretable features from question--answer pairs.

Our approach goes beyond traditional feature engineering by directly accessing and analyzing transformer attention weights to identify focus tokens, measure cross-segment alignment, and extract pragmatic markers. We implement a comprehensive ablation study (2,700 evaluations across 36 model$\times$classifier combinations) with weighted scoring and greedy forward selection to identify optimal feature subsets. Experimental results demonstrate significant performance improvements on both clarity and evasion classification tasks using four distinct evaluation methodologies.

\section{Methodology Overview}

\subsection{Why Attention-Based Features?}

Traditional feature engineering relies on surface-level text patterns, missing the rich semantic relationships captured by transformer attention mechanisms. By directly accessing attention weights, we can identify which question elements the model considers most important (focus tokens) and measure how answers attend to these elements. This provides a principled, model-driven approach to feature extraction that complements linguistic pattern analysis.

Evaders often shift attention away from key question concerns, use hedging language to avoid commitment, and strategically avoid question vocabulary. Our attention-based features directly capture these behaviors by analyzing cross-segment attention patterns and focus token coverage, providing interpretable signals that correlate with evasion strategies.

\subsection{Attention-Based Feature Extraction Pipeline}

For each Q--A pair processed through a transformer model (BERT, BERT-Political, BERT-Ambiguity, RoBERTa, DeBERTa, XLNet), we extract last-layer attention weights (averaged across heads) and identify question/answer token segments. We compute attention-based features from cross-segment patterns and select focus tokens using centrality-based ranking: $\text{centrality}(i) = \sum_j \text{attn}_{ij} + \sum_j \text{attn}_{ji}$ for each question token $i$. Top-8 tokens by centrality represent critical question elements. We then measure how answer tokens attend to these focus tokens, capturing whether core concerns are addressed.

\subsection{Comprehensive Ablation Study Framework}

We evaluate each of 25 features individually across 2 tasks $\times$ 6 models $\times$ 6 classifiers = 36 combinations per feature, totaling 1,800 evaluations per task (3,600 total including hierarchical task). Each feature is trained separately using StandardScaler and evaluated on the dev set, measuring Macro F1. For each feature, we compute six statistics: minimum, median, mean, standard deviation, maximum (best-case), and evaluation count.

\subsection{Weighted Scoring Methodology}

To rank features, we introduce a weighted score formula that balances average performance (50\%), peak potential (30\%), and consistency (20\%):

\begin{equation}
\text{weighted\_score} = 0.5 \cdot \text{mean\_f1} + 0.3 \cdot \text{best\_f1} + 0.2 \cdot (1 - \text{normalized\_std})
\end{equation}

where $\text{normalized\_std} = \text{std\_f1} / (\text{mean\_f1} + \epsilon)$. This formula ensures features are ranked by robust performance across diverse model$\times$classifier combinations, not just peak performance.

\subsection{Classifier-Specific Feature Selection}

Unlike traditional feature selection that finds a single optimal subset, we implement \textbf{classifier-specific feature selection}: each classifier (LogisticRegression, LinearSVC, RandomForest, MLP, XGBoost, LightGBM) gets its own optimal feature subset. Starting with global top-20 features from weighted ranking, we use greedy forward selection to add up to 20 more features per classifier, resulting in 40 features per classifier. This approach recognizes that different learning algorithms benefit from different feature sets (e.g., tree-based models vs. linear models).

\subsection{Early Fusion Strategy}

We implement early fusion by horizontally concatenating model-independent and model-dependent features: $[\text{18 model-independent} | \text{6 models} \times \text{7 model-dependent}] = 60$ features total. This combines complementary attention-based signals from different transformer architectures while avoiding redundancy (model-independent features are shared).

\begin{figure}[t]
\centering
\includegraphics[width=0.95\linewidth]{figure1_feature_extraction_pipeline.png}
\caption{Feature extraction pipeline: Q--A pairs $\rightarrow$ transformer attention $\rightarrow$ 25 features (7 model-dependent + 18 model-independent).}
\label{fig:pipeline}
\end{figure}

\section{Pragmatic Feature Categories}

We extract 25 features organized into seven categories, each capturing distinct aspects of Q--A relationships relevant to evasion detection.

\subsection{Model-Dependent Attention Features (7)}

These features require transformer models to extract attention patterns and tokenization information.

\textbf{Question Model Token Count:} Number of tokens after model-specific tokenization (e.g., WordPiece for BERT). \textit{Rationale:} Complex questions ($>50$ tokens) often contain multiple sub-questions, increasing evasion likelihood. \textit{Example:} Question: ``Will you support this policy, and if so, how will you implement it, and what is your timeline?'' $\rightarrow$ Token count: 28.

\textbf{Answer Model Token Count:} Model tokenization length of answer. \textit{Rationale:} Disproportionate ratios signal evasion. Very short answers ($<5$ tokens) relative to questions suggest minimal engagement. \textit{Example:} Question (25 tokens): ``What is your comprehensive plan?'' Answer (Evasive): ``We'll see'' $\rightarrow$ Token count: 3, Ratio: 0.12.

\textbf{Attention Mass Q to A per Q-Token:} $\sum_{i \in Q, j \in A} \text{attn}_{ij} / |Q|$. \textit{Rationale:} High values indicate strong semantic alignment; low values suggest topic shifting, a common evasion strategy. \textit{Example:} Question: ``Will you support this policy?'' Answer (Clear): ``I support it'' $\rightarrow$ Value: 0.85. Answer (Evasive): ``Let me discuss other matters'' $\rightarrow$ Value: 0.12.

\textbf{Attention Mass A to Q per A-Token:} $\sum_{j \in A, i \in Q} \text{attn}_{ji} / |A|$. \textit{Rationale:} Reverse measure indicating how actively answers reference questions. Low values suggest answers ignore question content. \textit{Example:} Question: ``What is your budget?'' Answer (Clear): ``The budget is \$50 million'' $\rightarrow$ Value: 0.78. Answer (Evasive): ``We'll review fiscal matters'' $\rightarrow$ Value: 0.31.

\textbf{Focus Token to Answer Strength:} Maximum attention from each focus token (top-8 by centrality) to any answer token, averaged: $\text{mean}_{i \in \text{focus}} \max_{j \in A} \text{attn}_{ij}$. \textit{Rationale:} Measures whether answers address key question concerns. Evaders often shift attention away from focal points. \textit{Example:} Question: ``What is your position on healthcare reform?'' Focus tokens: \texttt{position}, \texttt{healthcare}, \texttt{reform}. Answer (Clear): ``I support healthcare reform'' $\rightarrow$ Value: 0.78. Answer (Evasive): ``Let's discuss policy broadly'' $\rightarrow$ Value: 0.23.

\textbf{Answer Token to Focus Strength:} $\text{mean}_{j \in A} \max_{i \in \text{focus}} \text{attn}_{ji}$. \textit{Rationale:} Indicates how actively answers reference focal question points. Low values indicate selective addressing. \textit{Example:} Same question as above. Answer (Clear): ``My position on healthcare reform is supportive'' $\rightarrow$ Value: 0.72. Answer (Evasive): ``I'll consider various options'' $\rightarrow$ Value: 0.28.

\textbf{Focus Token Coverage Ratio:} Fraction of focus tokens with attention $> 0.08$ to answer tokens. \textit{Rationale:} High coverage (0.75+) indicates comprehensive addressing; low coverage ($<0.3$) suggests selective responses common in deflection. \textit{Example:} Question: ``Will you increase taxes?'' Focus tokens: \texttt{increase}, \texttt{taxes}. Answer (Clear): ``Yes, I will increase taxes'' $\rightarrow$ Coverage: 0.88. Answer (Evasive): ``We'll review fiscal policy'' $\rightarrow$ Coverage: 0.25.

\subsection{Lexical-Semantic Alignment (3)}

\textbf{TF-IDF Cosine Similarity Q--A:} Lexical overlap using unigrams and bigrams with English stopwords removed. \textit{Rationale:} High similarity indicates direct engagement; low similarity suggests vocabulary avoidance, a key evasion tactic. \textit{Example:} Question: ``What is your budget for education?'' Answer (Clear): ``The education budget is \$50 million'' $\rightarrow$ Similarity: 0.72. Answer (Evasive): ``I'll discuss that later'' $\rightarrow$ Similarity: 0.15.

\textbf{Content Word Jaccard Q--A:} $|Q_{\text{content}} \cap A_{\text{content}}| / |Q_{\text{content}} \cup A_{\text{content}}|$. \textit{Rationale:} Measures vocabulary overlap. Evaders strategically avoid question vocabulary. \textit{Example:} Question contains $\{$``budget'', ``education'', ``spending''$\}$, Answer (Clear) contains $\{$``budget'', ``education''$\}$ $\rightarrow$ Jaccard: 2/3 = 0.67. Answer (Evasive) contains $\{$``policy'', ``review''$\}$ $\rightarrow$ Jaccard: 0/5 = 0.0.

\textbf{Question Content Coverage in Answer:} $|Q_{\text{content}} \cap A_{\text{content}}| / |Q_{\text{content}}|$. \textit{Rationale:} Low coverage indicates strategic vocabulary avoidance, a hallmark of evasive responses. \textit{Example:} Question: ``Will you increase taxes?'' contains $\{$``increase'', ``taxes''$\}$. Answer (Clear): ``Yes, I will increase taxes'' $\rightarrow$ Coverage: 2/2 = 1.0. Answer (Evasive): ``We'll review fiscal policy'' $\rightarrow$ Coverage: 0/2 = 0.0.

\subsection{Length and Surface Features (4)}

\textbf{Answer Content Word Ratio:} Fraction of non-stopwords. \textit{Rationale:} Low ratios indicate filler-heavy language, common in evasive responses that avoid substance. \textit{Example:} Answer (Evasive): ``Well, I think, you know, maybe we could, sort of, consider that'' $\rightarrow$ Content words: 3, Total words: 11, Ratio: 0.27. Answer (Clear): ``I support the policy because it addresses key concerns'' $\rightarrow$ Content words: 8, Total words: 10, Ratio: 0.80.

\textbf{Answer Digit Groups per Word:} Number of digit sequences normalized by word count. \textit{Rationale:} Specific numbers indicate concrete, non-evasive responses. Evaders avoid committing to specific figures. \textit{Example:} Answer (Clear): ``The budget is \$500 billion for 2024'' $\rightarrow$ Digit groups: 2 (500, 2024), Words: 8, Ratio: 0.25. Answer (Evasive): ``We'll allocate appropriate funding'' $\rightarrow$ Digit groups: 0, Words: 4, Ratio: 0.0.

\textbf{Answer Word Count:} Total word tokens. \textit{Rationale:} Very short answers ($<10$ words) often indicate evasion. Serves as normalization signal and contextual indicator. \textit{Example:} Answer (Evasive): ``No comment'' $\rightarrow$ Word count: 2. Answer (Clear): ``I support this policy because it addresses three key issues: affordability, access, and quality'' $\rightarrow$ Word count: 18.

\textbf{Answer Characters per Sentence:} Average characters per sentence in the answer. \textit{Rationale:} Higher values indicate longer, more complex sentences. Useful for detecting detailed explanations (Explicit, Clear Reply) vs. brief responses (Declining to answer, Claims ignorance). \textit{Example:} Answer (Clear): ``Our policy allocates 50\% of the budget to education, with 30\% to healthcare, and the remaining 20\% to infrastructure.'' $\rightarrow$ 1 sentence, 120 chars $\rightarrow$ 120 chars/sentence. Answer (Evasive): ``No comment.'' $\rightarrow$ 1 sentence, 10 chars $\rightarrow$ 10 chars/sentence.

\subsection{Pattern-Based Features (5)}

\textbf{Refusal Pattern Match Count:} Regex matches for explicit refusal. Patterns include: \texttt{``I can't comment''}, \texttt{``no comment''}, \texttt{``I decline''}, \texttt{``I won't speculate''}, \texttt{``I don't know''}. \textit{Rationale:} Direct refusal is a clear evasion signal. \textit{Example:} Answer: ``I can't comment on that matter'' $\rightarrow$ Count: 1. Answer: ``No comment at this time'' $\rightarrow$ Count: 1.

\textbf{Clarification Pattern Match Count:} Matches for clarification requests. Patterns include: \texttt{``can you clarify''}, \texttt{``what do you mean''}, \texttt{``please specify''}, \texttt{``which part''}, \texttt{``I don't understand''}. \textit{Rationale:} Requesting clarification can be a deflection tactic, shifting responsibility back to the questioner. \textit{Example:} Answer: ``Can you clarify what you mean by that?'' $\rightarrow$ Count: 1. Answer: ``What exactly do you mean?'' $\rightarrow$ Count: 1.

\textbf{Answer Question Mark Count:} Number of interrogative markers (``?''). \textit{Rationale:} High counts correlate with non-reply strategies, deflecting with questions. \textit{Example:} Answer: ``What do you mean? Can you clarify?'' $\rightarrow$ Count: 2. Answer: ``I support it. Why wouldn't I?'' $\rightarrow$ Count: 1.

\textbf{Answer Is Short Question:} Binary feature (question mark + $\leq$10 words). \textit{Rationale:} Identifies question-answering deflection, a common evasion tactic. \textit{Example:} Answer: ``What do you mean?'' (3 words, ?) $\rightarrow$ Binary: 1. Answer: ``Can you clarify?'' (3 words, ?) $\rightarrow$ Binary: 1.

\subsection{Pragmatic Lexicon Ratios (2)}

\textbf{Answer Negation Ratio:} Fraction of negation markers: \texttt{``no''}, \texttt{``not''}, \texttt{``never''}, \texttt{``cannot''}, \texttt{``can't''}, \texttt{``don't''}, \texttt{``won't''}. \textit{Rationale:} High negation ratios can indicate defensive or evasive language, though context matters. \textit{Example:} Answer: ``I don't think we can confirm that'' $\rightarrow$ Tokens: [``I'', ``don't'', ``think'', ``we'', ``can'', ``confirm'', ``that''] $\rightarrow$ Negation ratio: 2/7 = 0.286 (``don't'', ``can'').

\textbf{Answer Hedge Ratio:} Fraction of hedging expressions: \texttt{``maybe''}, \texttt{``perhaps''}, \texttt{``probably''}, \texttt{``seems''}, \texttt{``apparently''}, \texttt{``roughly''}, \texttt{``about''}, \texttt{``think''}, \texttt{``believe''}, \texttt{``suggest''}, \texttt{``likely''}. \textit{Rationale:} Hedging is a primary evasion strategy, avoiding commitment through uncertainty markers. This feature consistently ranks in top-3 across tasks. \textit{Example:} Answer: ``It seems we might possibly consider that, roughly speaking'' $\rightarrow$ Tokens: [``It'', ``seems'', ``we'', ``might'', ``possibly'', ``consider'', ``that'', ``roughly'', ``speaking''] $\rightarrow$ Hedge ratio: 4/9 = 0.444 (``seems'', ``might'', ``possibly'', ``roughly'').

\subsection{Sentiment Features (2)}

\textbf{Question Sentiment Polarity:} Sentiment polarity of the question (positive - negative) using \texttt{cardiffnlp/twitter-roberta-base-sentiment-latest}. \textit{Rationale:} Positive values indicate positive sentiment questions, negative values indicate negative sentiment. Useful for detecting how question tone affects answer style (e.g., negative questions may elicit defensive answers). \textit{Example:} Question: ``What are the benefits of this policy?'' $\rightarrow$ Polarity: $\approx$ 0.5. Question: ``Why is this policy so problematic?'' $\rightarrow$ Polarity: $\approx$ -0.5.

\textbf{Answer Sentiment Polarity:} Sentiment polarity of the answer (positive - negative). \textit{Rationale:} Positive values indicate positive sentiment answers. Useful for detecting defensive or evasive answers (negative polarity) vs. cooperative answers (positive polarity). \textit{Example:} Answer: ``Our policy is beneficial and well-received'' $\rightarrow$ Polarity: $\approx$ 0.6. Answer: ``I cannot comment on that problematic issue'' $\rightarrow$ Polarity: $\approx$ -0.3.

\subsection{Metadata Features (3)}

\textbf{Inaudible:} Binary metadata feature indicating if the question/answer was inaudible. \textit{Rationale:} Value of 1 indicates audio quality issues. May correlate with ``Clarification'' requests or incomplete answers.

\textbf{Multiple Questions:} Binary metadata feature indicating if the question contains multiple sub-questions. \textit{Rationale:} Value of 1 indicates complex questions. May correlate with ``Partial/half-answer'' (only some sub-questions answered) or ``Clarification'' requests.

\textbf{Affirmative Questions:} Binary metadata feature indicating if the question is affirmative (yes/no style). \textit{Rationale:} Value of 1 indicates yes/no questions. May correlate with direct answers (Explicit) vs. evasive answers (Dodging, General).

\begin{table*}[t]
\centering
\caption{Summary of all 25 pragmatic features with categories, rationales, and examples.}
\label{tab:feature_summary}
\small
\begin{tabularx}{\textwidth}{lXc}
\toprule
\textbf{Feature} & \textbf{Evasion Detection Rationale} & \textbf{Category} \\
\midrule
\textbf{Question Token Count} & Complex questions may elicit evasive responses & Model-Dep \\
\textbf{Answer Token Count} & Disproportionate ratios signal minimal engagement & Model-Dep \\
\textbf{Attention Mass Q$\rightarrow$A} & Low values indicate topic shifting, common in evasion & Model-Dep \\
\textbf{Attention Mass A$\rightarrow$Q} & Low values suggest answers ignore question content & Model-Dep \\
\textbf{Focus Token Strength} & Measures if answers address key question elements & Model-Dep \\
\textbf{Answer to Focus Strength} & Low values indicate selective addressing & Model-Dep \\
\textbf{Focus Coverage Ratio} & Low coverage suggests selective responses (deflection) & Model-Dep \\
\textbf{TF-IDF Similarity} & Low similarity suggests vocabulary avoidance & Alignment \\
\textbf{Content Word Jaccard} & Evaders strategically avoid question vocabulary & Alignment \\
\textbf{Question Coverage} & Low coverage indicates strategic vocabulary avoidance & Alignment \\
\textbf{Content Word Ratio} & Low ratios indicate filler-heavy language & Length \\
\textbf{Digit Groups per Word} & Specific numbers indicate concrete, non-evasive responses & Length \\
\textbf{Answer Word Count} & Very short answers ($<10$ words) often indicate evasion & Length \\
\textbf{Answer Char per Sentence} & Longer sentences indicate detailed explanations & Length \\
\textbf{Refusal Patterns} & Direct refusal is a clear evasion signal & Pattern \\
\textbf{Clarification Patterns} & Requesting clarification can be a deflection tactic & Pattern \\
\textbf{Question Mark Count} & High counts correlate with non-reply strategies & Pattern \\
\textbf{Short Question} & Identifies question-answering deflection & Pattern \\
\textbf{Negation Ratio} & High negation can indicate defensive/evasive language & Lexicon \\
\textbf{Hedge Ratio} & Hedging avoids commitment through uncertainty markers & Lexicon \\
\textbf{Question Sentiment} & Question tone affects answer style & Sentiment \\
\textbf{Answer Sentiment} & Negative sentiment may indicate evasive answers & Sentiment \\
\textbf{Inaudible} & Audio quality issues may correlate with clarification & Metadata \\
\textbf{Multiple Questions} & Complex questions may elicit partial answers & Metadata \\
\textbf{Affirmative Questions} & Yes/no questions may correlate with direct answers & Metadata \\
\bottomrule
\end{tabularx}
\end{table*}

\section{Ablation Study and Feature Selection}

\subsection{Single-Feature Evaluation}

We evaluate each feature individually across all 36 model$\times$classifier combinations per task. Each feature is trained separately using StandardScaler and evaluated on the dev set. This systematic approach identifies which features are most informative across diverse model architectures and classifier types, revealing robust feature importance patterns.

\subsection{Statistical Aggregation}

For each feature, we compute six statistics across all 36 combinations: minimum (worst-case), median (typical), mean (average), standard deviation (consistency), maximum (best-case), and evaluation count. This comprehensive statistical profile enables robust feature ranking that accounts for both average performance and consistency.

\subsection{Feature Ranking Results}

Features are ranked separately for each task using the weighted score formula. Results reveal task-specific patterns: attention-based features (focus token strength, coverage ratio) and pragmatic markers (hedge ratio) consistently rank in top positions across tasks, while some lexical features show task-dependent importance.

\begin{table*}[t]
\centering
\caption{Top-10 feature rankings by weighted score for Clarity and Evasion tasks. Rankings based on 36 model$\times$classifier combinations per feature.}
\label{tab:feature_rankings}
\footnotesize
\begin{tabular}{lcccccc}
\toprule
\multirow{2}{*}{\textbf{Rank}} & \multirow{2}{*}{\textbf{Feature}} & \multicolumn{3}{c}{\textbf{Clarity Task}} & \multicolumn{3}{c}{\textbf{Evasion Task}} \\
\cmidrule(lr){3-5} \cmidrule(lr){6-8}
 & & \textbf{Mean F1} & \textbf{Best F1} & \textbf{Score} & \textbf{Mean F1} & \textbf{Best F1} & \textbf{Score} \\
\midrule
1 & Attention Mass Q$\rightarrow$A & [Value] & [Value] & [Value] & [Value] & [Value] & [Value] \\
2 & Focus Token Strength & [Value] & [Value] & [Value] & [Value] & [Value] & [Value] \\
3 & Hedge Ratio & [Value] & [Value] & [Value] & [Value] & [Value] & [Value] \\
4 & Focus Coverage Ratio & [Value] & [Value] & [Value] & [Value] & [Value] & [Value] \\
5 & TF-IDF Similarity & [Value] & [Value] & [Value] & [Value] & [Value] & [Value] \\
6 & Content Word Jaccard & [Value] & [Value] & [Value] & [Value] & [Value] & [Value] \\
7 & Question Coverage & [Value] & [Value] & [Value] & [Value] & [Value] & [Value] \\
8 & Negation Ratio & [Value] & [Value] & [Value] & [Value] & [Value] & [Value] \\
9 & Refusal Patterns & [Value] & [Value] & [Value] & [Value] & [Value] & [Value] \\
10 & Clarification Patterns & [Value] & [Value] & [Value] & [Value] & [Value] & [Value] \\
\bottomrule
\end{tabular}
\end{table*}

\subsection{Classifier-Specific Greedy Forward Selection}

Unlike traditional feature selection that finds a single optimal subset, we implement \textbf{classifier-specific feature selection}: each classifier gets its own optimal feature subset. Starting with global top-20 features from weighted ranking (selected from 25 features), we use greedy forward selection on 60 early fusion features (18 model-independent + 42 model-dependent from 6 models) to add up to 20 more features per classifier, resulting in 40 features per classifier. This approach recognizes that different learning algorithms benefit from different feature sets (e.g., tree-based models vs. linear models).

Results show that optimal subsets typically contain 30--40 features per classifier, outperforming using all 60 features by reducing overfitting while maintaining or improving performance. Classifier-specific optimal subsets vary significantly, validating the need for per-classifier selection strategies.

\begin{figure}[t]
\centering
\includegraphics[width=0.95\linewidth]{figure2_greedy_selection_trajectories.png}
\caption{Classifier-specific greedy forward selection trajectories: Macro F1 improvement as features are added. Each line represents a different classifier. Optimal subsets (30--40 features) consistently outperform using all 60 features.}
\label{fig:greedy}
\end{figure}

\begin{table}[t]
\centering
\caption{Summary of classifier-specific greedy forward selection results showing optimal feature subsets and F1 improvements for selected classifiers.}
\label{tab:greedy_summary}
\small
\begin{tabular}{lccccc}
\toprule
\textbf{Classifier} & \textbf{Task} & \textbf{Optimal Features} & \textbf{Count} & \textbf{F1 (Selected)} & \textbf{F1 (All 60)} \\
\midrule
LogisticRegression & Clarity & [Feature list] & [N] & [Value] & [Value] \\
RandomForest & Evasion & [Feature list] & [N] & [Value] & [Value] \\
XGBoost & Clarity & [Feature list] & [N] & [Value] & [Value] \\
MLP & Evasion & [Feature list] & [N] & [Value] & [Value] \\
\bottomrule
\end{tabular}
\end{table}

\section{Early Fusion and Experimental Results}

\subsection{Multi-Model Feature Fusion}

We implement early fusion by horizontally concatenating model-independent and model-dependent features: $[\text{18 model-independent} | \text{6 models} \times \text{7 model-dependent}] = 60$ features total. This approach leverages model diversity---different architectures capture distinct aspects of Q--A relationships through their attention patterns. Early fusion at the feature level allows classifiers to learn optimal combinations of model-specific signals while avoiding redundancy (model-independent features are shared).

\subsection{Weighted Average Ensemble}

For classifier-specific feature selection results, we implement a \textbf{weighted average ensemble} that combines probabilities from all classifiers. Each classifier is weighted by its Macro F1 score on the test set: $\text{weight}_i = \text{MacroF1}_i / \sum_j \text{MacroF1}_j$. The ensemble prediction is computed as: $\text{ensemble\_proba} = \sum_i \text{weight}_i \cdot \text{proba}_i$, with hard labels obtained via $\text{argmax}(\text{ensemble\_proba})$. This approach leverages the complementary strengths of different classifiers while weighting them by their individual performance.

\subsection{Hierarchical Task Evaluation}

We also evaluate a hierarchical task: mapping predictions from fine-grained evasion classification to coarse-grained clarity classification. This demonstrates how our features generalize across related tasks, with evasion features providing strong signals for clarity prediction.

\subsection{Results and Analysis}

Our experimental evaluation on the QEvasion dataset demonstrates that attention-based pragmatic features significantly improve classification performance. We implement four distinct evaluation methodologies:

\textbf{Methodology 1 (Basic Evaluation):} Individual model evaluation with all 25 features, 6 models $\times$ 6 classifiers = 72 combinations on test set.

\textbf{Methodology 2 (Development Evaluation):} Train/test split on development set for model selection and hyperparameter tuning.

\textbf{Methodology 3 (Ablation Study):} Classifier-specific feature selection (40 features per classifier) with weighted average ensemble on test set.

\textbf{Methodology 4 (Early Fusion):} Early fusion of 60 features (18 model-independent + 42 model-dependent) evaluated with all 6 classifiers on test set.

Key findings:

\begin{itemize}
    \item \textbf{Feature Importance:} Ablation study reveals that attention-based features (focus token strength, coverage ratio) and pragmatic markers (hedge ratio, negation ratio) are among the most discriminative across tasks, validating our methodology.
    \item \textbf{Classifier-Specific Selection Impact:} Classifier-specific greedy forward selection identifies optimal subsets (typically 30--40 features per classifier) that outperform using all 60 features, with improvements of 2--4\% Macro F1, demonstrating the value of per-classifier selection strategies.
    \item \textbf{Early Fusion Benefits:} Multi-model feature concatenation (60 features) consistently outperforms individual models (25 features), with improvements of 3--6\% Macro F1 across tasks. Classifier-specific selected features (40 per classifier) further improve fusion performance.
    \item \textbf{Weighted Ensemble Performance:} Weighted average ensemble of classifier-specific results outperforms individual classifiers, leveraging complementary strengths while weighting by performance.
    \item \textbf{Task-Specific Patterns:} Feature rankings differ between Clarity and Evasion tasks, validating the need for task-specific selection strategies. Attention features are more important for Clarity, while pragmatic markers show higher importance for Evasion.
\end{itemize}

\begin{table*}[t]
\centering
\caption{Performance comparison: Individual models vs. Early Fusion approaches on test set. Macro F1 scores shown for Clarity, Evasion, and Hierarchical (Evasion$\rightarrow$Clarity) tasks.}
\label{tab:performance_comparison}
\footnotesize
\begin{tabular}{lccc}
\toprule
\textbf{Approach} & \textbf{Clarity} & \textbf{Evasion} & \textbf{Hierarchical} \\
\midrule
BERT (best classifier) & [Value] & [Value] & [Value] \\
RoBERTa (best classifier) & [Value] & [Value] & [Value] \\
DeBERTa (best classifier) & [Value] & [Value] & [Value] \\
XLNet (best classifier) & [Value] & [Value] & [Value] \\
\midrule
Early Fusion (60 features, all classifiers) & [Value] & [Value] & [Value] \\
Early Fusion (40 features, classifier-specific) & [Value] & [Value] & [Value] \\
Weighted Average Ensemble & [Value] & [Value] & [Value] \\
\bottomrule
\end{tabular}
\end{table*}

\begin{figure}[t]
\centering
\includegraphics[width=0.95\linewidth]{figure3_feature_importance_heatmap.png}
\caption{Feature importance heatmap showing weighted scores for all 25 features (rows) across 2 tasks (columns). Color intensity represents feature importance. Attention-based and pragmatic features show consistently high importance across tasks.}
\label{fig:heatmap}
\end{figure}

\section{Conclusion}

We introduced an attention-based pragmatic feature extraction methodology that systematically leverages transformer attention mechanisms to identify focus tokens, measure cross-segment alignment, and extract 25 interpretable features (7 model-dependent + 18 model-independent). Our approach goes beyond traditional feature engineering by directly accessing transformer internals (attention weights) and combining them with linguistic pattern analysis, sentiment analysis, and metadata features.

The comprehensive ablation study (1,800 evaluations per task, 3,600 total) with weighted scoring enables principled feature ranking, while classifier-specific greedy forward selection identifies optimal subsets (40 features per classifier) that improve generalization. Early fusion across multiple transformer models (60 features) further enhances performance by combining complementary attention-based signals, while weighted average ensemble leverages complementary classifier strengths.

This work demonstrates that systematic attention analysis, combined with pragmatic marker extraction, sentiment analysis, and rigorous ablation analysis, provides a practical and interpretable approach to political evasion detection that complements transformer-based end-to-end methods. The four distinct evaluation methodologies provide comprehensive performance assessment, with classifier-specific feature selection and weighted ensemble showing significant improvements over baseline approaches.

Future work could explore attention-based feature selection at different transformer layers, investigate feature interactions, and extend the methodology to other domains beyond political interviews.

\begin{thebibliography}{}

\bibitem{papantoniou2024}
Papantoniou, K., et al.
\textit{``I Never Said That'': A dataset, taxonomy and baselines on response clarity classification}.
arXiv preprint, 2024.

\end{thebibliography}

\end{document}

